algorithm: 'PPO'
actor_learning_rate : 0.00005
critic_learning_rate: 0.0001
density_learning_rate: 0.0001
gamma: 0.99
lamda: 0.97
eps: 0.2
hidden_dim: 128
entropy_coef: 0.01
epochs: 20
buffer_batchsize: 32
sample_batchsize: 64
episodes: 500
offset: 5
beta_coef: 0.5
kld_weight: 0.00025
